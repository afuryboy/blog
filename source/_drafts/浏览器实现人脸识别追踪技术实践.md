---
title: 浏览器实现人脸识别追踪技术实践
date: 2019-07-14 22:19:34
tags:
notshow: true
---

# 前言
> 在AI大行其道的今天，许多AI技术如雨后春笋般涌现，作为web前端开发，貌似离ai较远，其实不然，今天给大家讲一个关于基于浏览器实现人群洞察的demo


# AI真香

手动献上视频

<video width="480" height="320" controls>
<source src="b.mp4">
</video>

# 爬坑指南
>做成这个demo前期也是经历很多坑，包括环验证可行性、开发环境问题、ui稿、分辨率适配、浏览器兼容问题，这里不做细讲

![description](cry.png)



# 验证环节

最初打算使用webRTC技术获取实时视频流数据，验证过程发现电脑摄像头无法同时用2种语言（前端js获取视频流，底层人脸识别sdk读取实时的视频进而输出人脸框及属性）去读取视频流数据，只能退而求其次使用图片来模拟视频。

第一次实践： 图片流

对应的web界面输出从视频流改为图片url流，显然可预见的是画面流畅度不高，从产品体验角度实际确实挺卡顿的，中间尝试发送base64的纯图片流后依旧卡顿明显。无法满足产品需求，只能更换技术方案。

后面就有人提出虚化摄像头的想法，大家论证可行，于是立马实践。发现确实可以
后续：从图片流重新转回webRTC

# 流程图

完整的流程图如下：

![description](2.png)

前端工作流程：

![description](3.png)

# 人脸追踪
> sdk读取是720p的画面数据, 而实际界面必须适配16:9的主流分辨率(720p、1080p、4k),所以人脸框追踪需要通过计算来实现等比缩放,一下是中心缩放算法:

![description](5.png)

# 走出国门
![description](4.jpeg)

后续也开发了不同需求的版本，比如：国际化版、人脸图像缓存3分钟版

# 源码

哈哈，此处略去一万字的源码...，毕竟公司项目，还是要遵守一下职业操守。不过可以简单的介绍一下大概使用的技术和策略。

1、后端通过人脸sdk输出的数据，包装好数据每30毫秒吐一次给前端。

2、前端通过vue开发界面，人脸框等实时数据就是一个个div标签渲染出来的，并没有使用canvas,并且移动过程中人脸框的贴合也都很ok,不会有滞后的感觉，这里给vue点个赞👍

3、前端通过websocket与后端建立长连接，实时输出数据。

4、webRTC API:通过 navigator.mediaDevices.getUserMedia获取摄像头视频流数据；把视频流stream 赋值给video标签的 dom的srcObject属性。即可展示视频。

5、sdk读取是720p的画面数据，而我们的设备可能是16:9尺寸的不同分辨率的屏幕。所以这里采取中心缩放的策略，来将原本720p尺寸下的人脸框尺寸缩放到合适的大小。

